import os
import sys
import pickle
import re
from typing import List, Dict, Optional
import torch

# å¼•å…¥æ ¸å¿ƒåº“
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from fastbm25 import fastbm25
from sentence_transformers import CrossEncoder

# ğŸ”¥ğŸ”¥ğŸ”¥ æ–°å¢ï¼šå¯¼å…¥ç»Ÿä¸€é…ç½®æ–‡ä»¶ ğŸ”¥ğŸ”¥ğŸ”¥
try:
    import config
except ImportError:
    # å…œåº•ï¼šå¦‚æœæ‰¾ä¸åˆ°configï¼Œå°è¯•æŠŠå½“å‰ç›®å½•åŠ å…¥pathå†å¯¼å…¥
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    import config

class MoutaiRAGEngine:
    """
    RAG æ ¸å¿ƒå¼•æ“ç±»ï¼šè´Ÿè´£åŠ è½½æ¨¡å‹ã€ç´¢å¼•å’Œæ‰§è¡Œæœç´¢ã€‚
    """

    def __init__(self):
        # ğŸ”¥ ä¿®æ”¹ç‚¹ 1ï¼šç›´æ¥ä» config è¯»å–è·¯å¾„ï¼Œä¸å†æ‰‹åŠ¨æ‹¼æ¥
        self.index_dir = config.FAISS_INDEX_PATH
        self.docs_path = config.DOCS_INFO_PATH

        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸš€ [Engine] åˆå§‹åŒ–ä¸­... è®¾å¤‡: {self.device}")

        self.embeddings = None
        self.reranker = None
        self.vector_store = None
        self.bm25 = None
        self.all_documents = []
        self.is_ready = False

        self._load_resources()

    def _load_resources(self):
        try:
            # 1. åŠ è½½ Embedding
            print("ğŸ“¦ Loading Embeddings (BGE-Large)...")
            self.embeddings = HuggingFaceBgeEmbeddings(
                model_name="BAAI/bge-large-zh-v1.5",
                model_kwargs={'device': self.device},
                encode_kwargs={'normalize_embeddings': True}
            )

            # 2. åŠ è½½ Reranker
            print("ğŸ“¦ Loading Reranker (BGE-Reranker)...")
            self.reranker = CrossEncoder(model_name_or_path="BAAI/bge-reranker-large", device=self.device)

            # 3. æ£€æŸ¥å¹¶åŠ è½½ç´¢å¼•
            print(f"ğŸ“‚ Loading Indices from: {self.index_dir}")
            if not os.path.exists(self.index_dir):
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç´¢å¼•ç›®å½•: {self.index_dir}\nè¯·å…ˆè¿è¡Œ build_rag_index.py æ„å»ºç´¢å¼•ï¼")

            self.vector_store = FAISS.load_local(
                self.index_dir,
                self.embeddings,
                allow_dangerous_deserialization=True
            )

            # 4. åŠ è½½åŸå§‹æ–‡æ¡£ docs.pkl
            if not os.path.exists(self.docs_path):
                raise FileNotFoundError(f"å…³é”®æ–‡ä»¶ç¼ºå¤±: {self.docs_path}")

            print(f"ğŸ“‚ Loading Documents from: {self.docs_path}")
            with open(self.docs_path, 'rb') as f:
                self.all_documents = pickle.load(f)

            # 5. é‡å»º BM25
            print("ğŸ”„ Rebuilding BM25 Index...")
            corpus_texts = [doc.page_content for doc in self.all_documents]
            self.bm25 = fastbm25(corpus_texts)

            self.is_ready = True
            print(f"âœ… RAG å¼•æ“åŠ è½½å®Œæˆ! åŒ…å« {len(self.all_documents)} æ¡æ•°æ®ã€‚")

        except Exception as e:
            print(f"âŒ RAG å¼•æ“åŠ è½½å¤±è´¥: {e}")
            # print("ğŸ’¡ æç¤º: å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œè¯·ç¡®ä¿ data/ ç›®å½•ä¸‹æœ‰ docs.pkl å’Œ faiss_index æ–‡ä»¶å¤¹")
            self.is_ready = False

    def _clean_text(self, text: str) -> str:
        """
        ğŸ§¹ æ–‡æœ¬æ¸…æ´—å™¨ï¼šä¿®å¤ PDF è§£æé—®é¢˜ï¼Œä½†ä¿æŠ¤è´¢åŠ¡æ•°å­—æ ¼å¼
        """
        if not text:
            return ""

        # 1. å…ˆä¿æŠ¤è´¢åŠ¡æ•°å­—æ ¼å¼ (å¦‚ "14,769,360.50" æˆ– "1,234.56")
        protected_numbers = []

        def protect_num(match):
            protected_numbers.append(match.group(0))
            return f"__NUM_{len(protected_numbers) - 1}__"

        text = re.sub(r'\d{1,3}(,\d{3})+(\.\d+)?', protect_num, text)

        # 2. æŠŠæ‰€æœ‰æ¢è¡Œç¬¦æ›¿æ¢æˆç©ºæ ¼
        text = text.replace('\n', ' ').replace('\r', ' ')

        # 3. ä¿®å¤è¢«ç©ºæ ¼æ‰“æ–­çš„ä¸­æ–‡è¯æ±‡
        text = re.sub(r'([\u4e00-\u9fa5])\s+([\u4e00-\u9fa5])', r'\1\2', text)

        # 4. åˆå¹¶ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)

        # 5. æ¢å¤ä¿æŠ¤çš„æ•°å­—
        for i, num in enumerate(protected_numbers):
            text = text.replace(f"__NUM_{i}__", num)

        return text.strip()

    def _extract_financial_highlights(self, text: str) -> List[str]:
        """ğŸ” æå–è´¢åŠ¡æ•°æ®äº®ç‚¹"""
        highlights = []
        patterns = [
            r'(è¥ä¸šæ”¶å…¥|è¥æ”¶|åˆ©æ¶¦æ€»é¢|å‡€åˆ©æ¶¦|å‘ç”Ÿé¢).*?([\d,]+\.?\d*)\s*(ä¸‡å…ƒ|äº¿å…ƒ|å…ƒ)',
            r'([\d,]+\.?\d*)\s*(ä¸‡å…ƒ|äº¿å…ƒ|å…ƒ).*?(è¥ä¸šæ”¶å…¥|è¥æ”¶|åˆ©æ¶¦æ€»é¢|å‡€åˆ©æ¶¦|å‘ç”Ÿé¢)',
            r'(?:å¢é•¿|ä¸‹é™|åŒæ¯”|è¾ƒä¸Šå¹´).*?(\d+\.?\d*)%\s*(?:å¢é•¿|ä¸‹é™|åŒæ¯”)?',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                clean_match = ''.join([m for m in match if m]).strip()
                if clean_match and clean_match not in highlights:
                    highlights.append(clean_match)

        return highlights[:3]

    def _rrf_fusion(self, list1, list2, k=60):
        """RRF èåˆç®—æ³•"""
        fusion_scores = {}
        for rank, (content, _) in enumerate(list1):
            if content not in fusion_scores: fusion_scores[content] = 0
            fusion_scores[content] += 1 / (rank + k)
        for rank, (content, _) in enumerate(list2):
            if content not in fusion_scores: fusion_scores[content] = 0
            fusion_scores[content] += 1 / (rank + k)
        return sorted(fusion_scores.items(), key=lambda x: x[1], reverse=True)

    def search(self, query: str, top_k: int = None) -> str:
        """æ‰§è¡Œæ··åˆæ£€ç´¢ + é‡æ’åº"""
        if not self.is_ready:
            return "âŒ é”™è¯¯: æœ¬åœ°è´¢åŠ¡æŠ¥è¡¨å¼•æ“æœªå°±ç»ªã€‚"
        
        # ğŸ”¥ ä¿®æ”¹ç‚¹ 2ï¼šé»˜è®¤ä½¿ç”¨ config ä¸­çš„ Top K
        if top_k is None:
            top_k = config.RAG_TOP_K if hasattr(config, 'RAG_TOP_K') else 50

        try:
            # Step 1: BM25 + Vector æ£€ç´¢ (ç²—æ’)
            bm25_res = self.bm25.top_k_sentence(query, k=top_k)
            bm25_list = [(res[0], res[2]) for res in bm25_res]
            vector_res = self.vector_store.similarity_search_with_score(query, k=top_k)
            vector_list = [(doc.page_content, score) for doc, score in vector_res]

            # Step 2: RRF èåˆ
            fusion_results = self._rrf_fusion(bm25_list, vector_list)
            candidate_texts = [item[0] for item in fusion_results[:top_k]]

            if not candidate_texts:
                return "æœªæ‰¾åˆ°ç›¸å…³æœ¬åœ°è´¢åŠ¡ä¿¡æ¯ã€‚"

            # Step 3: Rerank é‡æ’åº (ç²¾æ’)
            rerank_pairs = [[query, text] for text in candidate_texts]
            scores = self.reranker.predict(rerank_pairs, batch_size=4, show_progress_bar=False)

            scored_results = sorted(zip(candidate_texts, scores), key=lambda x: x[1], reverse=True)

            # Step 4: ç»„è£…ä¸Šä¸‹æ–‡
            content_map = {doc.page_content: doc.metadata for doc in self.all_documents}
            final_output = ["ä»¥ä¸‹æ˜¯ä»èŒ…å°å†å²å¹´æŠ¥ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼š"]

            # åªå–å‰ 5 ä¸ªæœ€ç›¸å…³çš„ç»“æœ
            for i, (text, score) in enumerate(scored_results[:5]):
                meta = content_map.get(text, {})
                source = meta.get('source', 'æœªçŸ¥å¹´æŠ¥')
                cleaned_text = self._clean_text(text)
                highlights = self._extract_financial_highlights(cleaned_text)
                highlight_str = f" [å…³é”®æ•°æ®: {' | '.join(highlights)}]" if highlights else ""

                final_output.append(f"èµ„æ–™[{i + 1}] æ¥æº: {source}{highlight_str}\nå†…å®¹: {cleaned_text}\n")

            return "\n".join(final_output)

        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"æ£€ç´¢è¿‡ç¨‹å‡ºé”™: {str(e)}"