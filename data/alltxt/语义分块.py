import os
import re
import json
import itertools
from loguru import logger
from fastbm25 import fastbm25
from typing import List, Dict, Tuple, Optional
# LangChain æ ¸å¿ƒç»„ä»¶
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

# å‘é‡æ•°æ®åº“ä¸æ¨¡å‹
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

# Reranker (ä½¿ç”¨ sentence_transformers åŸç”Ÿè°ƒç”¨ï¼Œæ¯” LangChain å°è£…æ›´çµæ´»)
from sentence_transformers import CrossEncoder

# æœ¬åœ°é…ç½®
import config as cfg


class AnnualReportChunker:
    """å¹´æŠ¥ç»“æ„åŒ–åˆ†å—å™¨"""

    def __init__(self):
        # å¹´æŠ¥å¸¸è§ç« èŠ‚æ ‡é¢˜æ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        self.section_patterns = [
            # ç¬¬XèŠ‚ æ ¼å¼ï¼ˆæœ€æ˜ç¡®ï¼‰
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+èŠ‚\s+\S.*$',
            r'^ç¬¬\d+\s*èŠ‚\s+\S.*$',

            # Markdownæ ‡é¢˜æ ¼å¼
            r'^#+\s+\S.*$',  # # æ ‡é¢˜

            # ä¸€çº§æ ‡é¢˜ï¼šä¸­æ–‡æ•°å­— + ã€
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+[ã€ï¼.]\s+\S.*$',  # ç¡®ä¿æ ‡é¢˜åæœ‰å†…å®¹

            # ç›®å½•/é‡è¦æç¤º/é‡Šä¹‰ï¼ˆå•ç‹¬æˆè¡Œï¼‰
            r'^é‡è¦æç¤º\s*$',
            r'^ç›®å½•\s*$',
            r'^é‡Šä¹‰\s*$',

            # å¸¸è§ç« èŠ‚åç§°ï¼ˆå®Œæ•´åŒ¹é…ï¼‰
            r'^å…¬å¸ç®€ä»‹\s*$',
            r'^ä¼šè®¡æ•°æ®\s*$',
            r'^è´¢åŠ¡æŠ¥å‘Š\s*$',
            r'^è‘£äº‹ä¼šæŠ¥å‘Š\s*$',
            r'^ç›‘äº‹ä¼šæŠ¥å‘Š\s*$',
            r'^é‡è¦äº‹é¡¹\s*$',
            r'^è‚¡æœ¬å˜åŠ¨\s*$',
            r'^è‚¡ä¸œä¿¡æ¯\s*$',
            r'^å…¬å¸å€ºåˆ¸\s*$',
            r'^è´¢åŠ¡æŠ¥è¡¨\s*$',
        ]

        # å­ç« èŠ‚æ ‡é¢˜æ¨¡å¼ï¼ˆç”¨äºè¯†åˆ«å°èŠ‚ï¼‰
        self.subsection_patterns = [
            r'^#+\s+\S.*$',  # Markdownæ ‡é¢˜
            r'^[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[)ï¼‰]\s*.+',
            r'^[ï¼ˆ(]\d+[)ï¼‰]\s*.+',
            r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s*.+',
        ]

    def is_section_title(self, line: str) -> Tuple[bool, str]:
        """
        åˆ¤æ–­ä¸€è¡Œæ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
        è¿”å›: (æ˜¯å¦æ˜¯æ ‡é¢˜, æ ‡é¢˜çº§åˆ«: 'main'/'sub'/'none')
        """
        line = line.strip()

        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸€çº§ç« èŠ‚æ ‡é¢˜
        for pattern in self.section_patterns:
            if re.match(pattern, line):
                return True, 'main'

        # æ£€æŸ¥æ˜¯å¦æ˜¯å­ç« èŠ‚æ ‡é¢˜
        for pattern in self.subsection_patterns:
            if re.match(pattern, line):
                return True, 'sub'

        return False, 'none'

    def clean_line(self, line: str) -> str:
        """æ¸…ç†è¡Œå†…å®¹"""
        line = line.strip()
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        line = re.sub(r'\s+', ' ', line)
        return line

    def is_markdown_metadata(self, line: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯Markdownå…ƒæ•°æ®"""
        line = line.strip()
        return line.startswith('---') or line.startswith('```') or line.startswith('|--')

    def extract_sections_from_text(self, text: str) -> List[Dict]:
        """
        ä»çº¯æ–‡æœ¬ä¸­æå–ç« èŠ‚
        è¿”å›: [{'title': ç« èŠ‚æ ‡é¢˜, 'level': çº§åˆ«, 'content': å†…å®¹, 'lines': è¡Œå·åˆ—è¡¨}]
        """
        lines = text.split('\n')
        sections = []
        current_section = None
        line_number = 0

        for line in lines:
            line_number += 1
            line = self.clean_line(line)

            # è·³è¿‡ç©ºè¡Œå’ŒMarkdownå…ƒæ•°æ®
            if not line or self.is_markdown_metadata(line):
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
            is_title, level = self.is_section_title(line)

            if is_title:
                # ä¿å­˜ä¸Šä¸€ä¸ªç« èŠ‚
                if current_section:
                    sections.append(current_section)

                # åˆ›å»ºæ–°ç« èŠ‚
                current_section = {
                    'title': line,
                    'level': level,
                    'content': '',
                    'lines': [line_number],
                    'char_start': len(text[:text.index(line)]) if line in text else 0
                }
            else:
                # æ·»åŠ åˆ°å½“å‰ç« èŠ‚
                if current_section:
                    current_section['content'] += line + '\n'
                    current_section['lines'].append(line_number)
                else:
                    # æ–‡æ¡£å¼€å¤´çš„å†…å®¹ï¼ˆåœ¨ç¬¬ä¸€ä¸ªç« èŠ‚ä¹‹å‰çš„ï¼‰
                    current_section = {
                        'title': 'æ–‡æ¡£å¼€å¤´',
                        'level': 'main',
                        'content': line + '\n',
                        'lines': [line_number],
                        'char_start': 0
                    }

        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_section:
            sections.append(current_section)

        return sections

    def merge_small_sections(self, sections: List[Dict], min_chars: int = 200) -> List[Dict]:
        """
        åˆå¹¶è¿‡å°çš„ç« èŠ‚åˆ°ç›¸é‚»ç« èŠ‚
        min_chars: æœ€å°å­—ç¬¦æ•°ï¼Œå°äºæ­¤å€¼çš„ç« èŠ‚ä¼šè¢«åˆå¹¶
        """
        if not sections:
            return sections

        merged = []
        i = 0

        while i < len(sections):
            current = sections[i]

            # å¦‚æœå½“å‰ç« èŠ‚å¤ªå°ä¸”ä¸æ˜¯ç¬¬ä¸€ä¸ªï¼Œå°è¯•åˆå¹¶åˆ°å‰ä¸€ä¸ªç« èŠ‚
            if len(current['content']) < min_chars and merged:
                merged[-1]['content'] += '\n\n' + current['content']
                merged[-1]['title'] += ' + ' + current['title']
                merged[-1]['lines'].extend(current['lines'])
            else:
                merged.append(current)

            i += 1

        return merged

    def chunk_by_sections(
            self,
            text: str,
            min_chars: int = 100,
            max_chars: int = 3000,
            merge_small: bool = True
    ) -> List[Dict]:
        """
        æŒ‰ç« èŠ‚è¿›è¡Œåˆ†å—

        å‚æ•°:
            text: è¾“å…¥æ–‡æœ¬
            min_chars: æœ€å°å­—ç¬¦æ•°ï¼ˆç”¨äºåˆå¹¶å°ç« èŠ‚ï¼‰
            max_chars: æœ€å¤§å­—ç¬¦æ•°ï¼ˆè¶…è¿‡æ­¤å¤§å°çš„ç« èŠ‚ä¼šè¿›ä¸€æ­¥åˆ†å‰²ï¼‰
            merge_small: æ˜¯å¦åˆå¹¶å°ç« èŠ‚

        è¿”å›:
            ç« èŠ‚å—åˆ—è¡¨
        """
        # æå–ç« èŠ‚
        sections = self.extract_sections_from_text(text)

        if merge_small:
            sections = self.merge_small_sections(sections, min_chars)

        # å¯¹è¿‡å¤§çš„ç« èŠ‚è¿›è¡Œåˆ†å‰²
        final_chunks = []
        for section in sections:
            if len(section['content']) <= max_chars:
                final_chunks.append(section)
            else:
                # åˆ†å‰²å¤§ç« èŠ‚
                sub_chunks = self.split_large_section(section, max_chars)
                final_chunks.extend(sub_chunks)

        return final_chunks

    def split_large_section(self, section: Dict, max_chars: int) -> List[Dict]:
        """
        å°†è¿‡å¤§çš„ç« èŠ‚åˆ†å‰²æˆå¤šä¸ªå—
        ç­–ç•¥ï¼š
        1. å…ˆæŒ‰å­ç« èŠ‚åˆ†å‰²ï¼ˆå¦‚æœæœ‰ï¼‰
        2. å†æŒ‰æ®µè½åˆ†å‰²
        3. é¿å…æˆªæ–­è¡¨æ ¼ï¼ˆæ£€æµ‹è¿ç»­çš„çŸ­è¡Œï¼‰
        """
        content = section['content']
        title = section['title']
        level = section['level']

        # æ£€æŸ¥æ˜¯å¦æœ‰Markdownæ ‡é¢˜ä½œä¸ºå­ç« èŠ‚
        lines = content.split('\n')
        has_subheadings = any(re.match(r'^#+\s+', line.strip()) for line in lines)

        if has_subheadings:
            return self._split_by_markdown_headings(section, max_chars)

        # æ²¡æœ‰å­ç« èŠ‚ï¼ŒæŒ‰æ™ºèƒ½æ®µè½åˆ†å‰²
        return self._split_by_smart_paragraphs(title, content, level, max_chars)

    def _split_by_markdown_headings(self, section: Dict, max_chars: int) -> List[Dict]:
        """æŒ‰Markdownæ ‡é¢˜åˆ†å‰²å¤§ç« èŠ‚"""
        content = section['content']
        title = section['title']
        level = section['level']

        chunks = []
        current_content = ''
        current_heading = title
        chunk_num = 1

        lines = content.split('\n')
        for line in lines:
            # æ£€æŸ¥æ˜¯å¦æ˜¯Markdownæ ‡é¢˜
            if re.match(r'^#+\s+', line.strip()):
                # ä¿å­˜å½“å‰å—
                if current_content.strip():
                    chunks.append({
                        'title': current_heading,
                        'content': current_content.strip(),
                        'level': level
                    })
                    chunk_num += 1

                # å¼€å§‹æ–°å—
                current_heading = f"{title} - {line.strip()}"
                current_content = line + '\n'
            else:
                current_content += line + '\n'

        # ä¿å­˜æœ€åä¸€å—
        if current_content.strip():
            chunks.append({
                'title': current_heading,
                'content': current_content.strip(),
                'level': level
            })

        return chunks

    def _split_by_smart_paragraphs(self, title: str, content: str, level: str, max_chars: int) -> List[Dict]:
        """
        æ™ºèƒ½æŒ‰æ®µè½åˆ†å‰²ï¼Œé¿å…æˆªæ–­è¡¨æ ¼
        è¡¨æ ¼ç‰¹å¾ï¼šè¿ç»­çš„çŸ­è¡Œï¼ˆé€šå¸¸<50å­—ç¬¦ï¼‰
        """
        lines = content.split('\n')
        chunks = []
        current_chunk_lines = []
        current_size = 0
        chunk_num = 1

        i = 0
        while i < len(lines):
            line = lines[i]
            line_size = len(line)

            # æ£€æµ‹Markdownè¡¨æ ¼
            is_markdown_table = line.strip().startswith('|')

            # æ£€æµ‹æ˜¯å¦æ˜¯è¡¨æ ¼ï¼ˆè¿ç»­çŸ­è¡Œæˆ–Markdownè¡¨æ ¼ï¼‰
            is_table = False
            if i + 3 < len(lines):  # è‡³å°‘4è¡Œ
                next_lines_short = all(len(lines[j]) < 80 for j in range(i, min(i + 4, len(lines))))
                next_lines_table = all(lines[j].strip().startswith('|') for j in range(i, min(i + 2, len(lines))))
                is_table = next_lines_short or next_lines_table

            # å¦‚æœåŠ ä¸Šè¿™ä¸€è¡Œä¼šè¶…é™
            if current_size + line_size > max_chars and current_chunk_lines:
                # å¦‚æœæ˜¯è¡¨æ ¼ï¼Œå°½é‡ä¿æŒè¡¨æ ¼å®Œæ•´
                if is_table or is_markdown_table:
                    # æ‰¾åˆ°è¡¨æ ¼ç»“æŸä½ç½®
                    table_end = i
                    while table_end < len(lines) and (
                            len(lines[table_end]) < 80 or lines[table_end].strip().startswith('|')):
                        table_end += 1

                    # å¦‚æœæ•´ä¸ªè¡¨æ ¼æ”¾å¾—ä¸‹ï¼Œä¸€èµ·æ”¾å…¥å½“å‰å—
                    table_size = sum(len(lines[j]) for j in range(i, table_end))
                    if current_size + table_size <= max_chars * 1.2:  # å…è®¸è¶…é™20%
                        # ä¸€èµ·æ”¾å…¥
                        for j in range(i, table_end):
                            current_chunk_lines.append(lines[j])
                            current_size += len(lines[j])
                        i = table_end
                        continue

                # ä¿å­˜å½“å‰å—
                chunks.append({
                    'title': f"{title} ({chunk_num})" if chunk_num > 1 else title,
                    'content': '\n'.join(current_chunk_lines),
                    'level': level
                })
                chunk_num += 1
                current_chunk_lines = []
                current_size = 0

            # æ·»åŠ åˆ°å½“å‰å—
            current_chunk_lines.append(line)
            current_size += line_size
            i += 1

        # ä¿å­˜æœ€åä¸€å—
        if current_chunk_lines:
            chunks.append({
                'title': f"{title} ({chunk_num})" if chunk_num > 1 else title,
                'content': '\n'.join(current_chunk_lines),
                'level': level
            })

        return chunks

    def chunk_by_sections_with_sliding_window(
            self,
            text: str,
            section_max_chars: int = 2000,
            sliding_window_size: int = 1000,
            sliding_overlap: int = 200,
            merge_small: bool = True
    ) -> List[Dict]:
        """
        æ··åˆåˆ†å—ç­–ç•¥ï¼šå…ˆç»“æ„åŒ–åˆ†å—ï¼Œå¤§ç« èŠ‚ä½¿ç”¨æ»‘çª—

        å‚æ•°:
            text: è¾“å…¥æ–‡æœ¬
            section_max_chars: ç« èŠ‚æœ€å¤§å­—ç¬¦æ•°ï¼Œè¶…è¿‡åˆ™ä½¿ç”¨æ»‘çª—
            sliding_window_size: æ»‘çª—å¤§å°
            sliding_overlap: æ»‘çª—é‡å å¤§å°
            merge_small: æ˜¯å¦åˆå¹¶å°ç« èŠ‚

        è¿”å›:
            åˆ†å—åˆ—è¡¨
        """
        # 1. å…ˆè¿›è¡Œç»“æ„åŒ–åˆ†å—
        sections = self.extract_sections_from_text(text)

        if merge_small:
            sections = self.merge_small_sections(sections, min_chars=100)

        # 2. å¯¹æ¯ä¸ªç« èŠ‚åˆ¤æ–­æ˜¯å¦éœ€è¦æ»‘çª—
        final_chunks = []
        for section in sections:
            content_len = len(section['content'])

            if content_len <= section_max_chars:
                # å°ç« èŠ‚ï¼Œç›´æ¥ä¿ç•™
                final_chunks.append(section)
            else:
                # å¤§ç« èŠ‚ï¼Œä½¿ç”¨æ»‘çª—åˆ†å—
                logger.info(f'ç« èŠ‚ "{section["title"][:30]}..." å¤§å° {content_len} å­—ç¬¦ï¼Œä½¿ç”¨æ»‘çª—åˆ†å—')

                sliding_chunks = self._sliding_window_by_char(
                    title=section['title'],
                    content=section['content'],
                    level=section['level'],
                    chunk_size=sliding_window_size,
                    overlap=sliding_overlap
                )

                final_chunks.extend(sliding_chunks)

        return final_chunks

    def _sliding_window_by_char(
            self,
            title: str,
            content: str,
            level: str,
            chunk_size: int,
            overlap: int
    ) -> List[Dict]:
        """
        æŒ‰å­—ç¬¦æ»‘çª—åˆ†å—
        ä¼˜å…ˆåœ¨å¥å­/æ®µè½è¾¹ç•Œåˆ‡åˆ†
        """
        chunks = []
        start = 0
        content_len = len(content)
        chunk_num = 1

        while start < content_len:
            # è®¡ç®—çª—å£ç»“æŸä½ç½®
            end = min(start + chunk_size, content_len)

            # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¥å­è¾¹ç•Œåˆ‡åˆ†
            if end < content_len:
                # ä¼˜å…ˆæ‰¾æ®µè½è¾¹ç•Œï¼ˆ\n\nï¼‰
                paragraph_boundary = content.rfind('\n\n', start, end)
                if paragraph_boundary > start + chunk_size * 0.7:  # è‡³å°‘ä¿ç•™70%
                    end = paragraph_boundary + 2
                else:
                    # å…¶æ¬¡æ‰¾å¥å­è¾¹ç•Œï¼ˆå¥å·ï¼‰
                    sentence_boundary = content.rfind('ã€‚', start, end)
                    if sentence_boundary > start + chunk_size * 0.7:
                        end = sentence_boundary + 1
                    else:
                        # æœ€åæ‰¾æ¢è¡Œ
                        line_boundary = content.rfind('\n', start, end)
                        if line_boundary > start + chunk_size * 0.7:
                            end = line_boundary + 1

            # æå–çª—å£å†…å®¹
            chunk_content = content[start:end].strip()

            if chunk_content:
                chunks.append({
                    'title': f"{title} (æ»‘åŠ¨{chunk_num})" if chunk_num > 1 else title,
                    'content': chunk_content,
                    'level': level,
                    'char_range': [start, end],
                    'overlap': overlap if chunk_num > 1 else 0
                })
                chunk_num += 1

            # ç§»åŠ¨çª—å£ï¼ˆä¿ç•™é‡å ï¼‰
            start = end - overlap if end < content_len else content_len

        return chunks


def load_md_file(file_path: str) -> List[str]:
    """
    åŠ è½½Markdownæ–‡ä»¶

    å‚æ•°:
        file_path: Markdownæ–‡ä»¶è·¯å¾„

    è¿”å›:
        å†…å®¹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€é¡µï¼ˆè¿™é‡Œå°†æ•´ä¸ªæ–‡ä»¶ä½œä¸ºä¸€é¡µå¤„ç†ï¼‰
    """
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()

        logger.info(f"æˆåŠŸåŠ è½½Markdownæ–‡ä»¶: {file_path} (å¤§å°: {len(content)} å­—ç¬¦)")
        return [content]  # è¿”å›åˆ—è¡¨æ ¼å¼ä»¥ä¿æŒæ¥å£ä¸€è‡´æ€§

    except FileNotFoundError:
        logger.error(f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return []
    except Exception as e:
        logger.error(f"è¯»å–Markdownæ–‡ä»¶å¤±è´¥: {e}")
        return []


def find_md_files(data_path: str) -> List[str]:
    """
    åœ¨æŒ‡å®šç›®å½•ä¸‹æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶

    å‚æ•°:
        data_path: æ•°æ®ç›®å½•è·¯å¾„

    è¿”å›:
        Markdownæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    md_files = []

    # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶
    if os.path.isfile(data_path) and data_path.endswith('.md'):
        return [data_path]

    # æ£€æŸ¥æ˜¯å¦ä¸ºç›®å½•
    if os.path.isdir(data_path):
        for file_name in os.listdir(data_path):
            if file_name.endswith('.md'):
                file_path = os.path.join(data_path, file_name)
                md_files.append(file_path)

    logger.info(f"åœ¨ {data_path} ä¸­æ‰¾åˆ° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶")
    return md_files


def chunk_md_by_sections(file_path: str) -> List[Dict]:
    """
    å¯¹Markdownæ–‡ä»¶æŒ‰ç« èŠ‚è¿›è¡Œåˆ†å—

    å‚æ•°:
        file_path: Markdownæ–‡ä»¶è·¯å¾„

    è¿”å›:
        ç« èŠ‚å—åˆ—è¡¨
    """
    chunker = AnnualReportChunker()
    pages = load_md_file(file_path)

    if not pages:
        logger.warning(f'æœªåŠ è½½åˆ°é¡µé¢å†…å®¹: {file_path}')
        return []

    # åˆå¹¶æ‰€æœ‰é¡µé¢æ–‡æœ¬
    full_text = '\n\n'.join(pages)

    # æŒ‰ç« èŠ‚åˆ†å—
    chunks = chunker.chunk_by_sections(
        full_text,
        min_chars=100,
        max_chars=3000,
        merge_small=True
    )

    return chunks


def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def get_text_chunks_from_md(file_path: str, save_json: bool = True) -> List[str]:
    """
    ä»Markdownæ–‡ä»¶è·å–æ–‡æœ¬åˆ†å—

    å‚æ•°:
        file_path: Markdownæ–‡ä»¶è·¯å¾„
        save_json: æ˜¯å¦ä¿å­˜ä¸ºJSONæ–‡ä»¶

    è¿”å›:
        æ ¼å¼åŒ–åçš„æ–‡æœ¬å—åˆ—è¡¨
    """
    pages = load_md_file(file_path)
    if not pages:
        return []

    full_text = "\n\n".join(pages)
    chunker = AnnualReportChunker()

    # æ‰§è¡Œç»“æ„åŒ–åˆ†å—
    structured_chunks = chunker.chunk_by_sections(
        full_text,
        min_chars=200,
        max_chars=800,
        merge_small=True
    )

    # --- ä¿å­˜ JSON æ–‡ä»¶ ---
    if save_json:
        # ä»æ–‡ä»¶è·¯å¾„ç”ŸæˆJSONæ–‡ä»¶å
        base_name = os.path.splitext(os.path.basename(file_path))[0]
        json_filename = f"{base_name}_chunks.json"

        # ä¿å­˜åˆ°åŒç›®å½•
        output_dir = os.path.dirname(file_path) or '.'
        output_path = os.path.join(output_dir, json_filename)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(structured_chunks, f, ensure_ascii=False, indent=4)
        logger.info(f"åˆ†å— JSON å·²ä¿å­˜è‡³: {output_path}")

    # æ ¼å¼è½¬æ¢ä¾›æ£€ç´¢ä½¿ç”¨
    final_text_list = []
    for item in structured_chunks:
        title = item.get('title', 'æœªçŸ¥ç« èŠ‚')
        content = item.get('content', '').strip()
        formatted_text = f"ã€ç« èŠ‚ï¼š{title}ã€‘\n{content}"
        final_text_list.append(formatted_text)

    logger.info(f"ç»“æ„åŒ–åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(final_text_list)} ä¸ªåˆ‡ç‰‡")
    return final_text_list


# ================= 1. æ ¸å¿ƒç®—æ³•: RRF èåˆ =================
def rrf_fusion(list1, list2, k=60):
    """
    Reciprocal Rank Fusion (RRF) ç®—æ³•
    å°†ä¸¤è·¯æ£€ç´¢ç»“æœï¼ˆBM25 å’Œ å‘é‡ï¼‰çš„æ’åè¿›è¡Œèåˆ
    """
    fusion_scores = {}

    # å»ºç«‹å†…å®¹åˆ°ç´¢å¼•çš„æ˜ å°„ï¼Œé˜²æ­¢é‡å¤å†…å®¹å¤„ç†
    # list1, list2 æ ¼å¼: [(content, score), (content, score)...]

    # å¤„ç†ç¬¬ä¸€è·¯ (BM25)
    for rank, (content, _) in enumerate(list1):
        if content not in fusion_scores: fusion_scores[content] = 0
        fusion_scores[content] += 1 / (rank + k)

    # å¤„ç†ç¬¬äºŒè·¯ (Vector)
    for rank, (content, _) in enumerate(list2):
        if content not in fusion_scores: fusion_scores[content] = 0
        fusion_scores[content] += 1 / (rank + k)

    # æŒ‰èåˆåˆ†æ•°ä»é«˜åˆ°ä½æ’åº
    # è¿”å›æ ¼å¼: [(content, fusion_score), ...]
    sorted_results = sorted(fusion_scores.items(), key=lambda x: x[1], reverse=True)
    return sorted_results


def hybrid_search_md(
        query: str,
        md_file_path: str,
        embedding_model: HuggingFaceEmbeddings,
        reranker_model: CrossEncoder
):
    """
    å¯¹Markdownæ–‡ä»¶è¿›è¡Œæ··åˆæ£€ç´¢

    å‚æ•°:
        query: æŸ¥è¯¢æ–‡æœ¬
        md_file_path: Markdownæ–‡ä»¶è·¯å¾„
        embedding_model: åµŒå…¥æ¨¡å‹
        reranker_model: é‡æ’åºæ¨¡å‹

    è¿”å›:
        æ£€ç´¢ç»“æœåˆ—è¡¨
    """
    import torch
    import gc

    # 0. åƒåœ¾å›æ”¶ï¼Œè…¾å‡ºç©ºé—´
    gc.collect()
    torch.cuda.empty_cache()

    logger.info(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {md_file_path}")

    # --- Step 1: è·å–åˆ‡ç‰‡ ---
    chunks = get_text_chunks_from_md(md_file_path)
    if not chunks:
        logger.error("æ–‡ä»¶å†…å®¹ä¸ºç©º")
        return []

    print(f"æ–‡æœ¬å…±åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")

    # --- Step 2: BM25 å¬å› ---
    print(">>> [1/4] æ­£åœ¨æ‰§è¡Œ BM25 å…³é”®è¯æ£€ç´¢...")
    bm25_model = fastbm25(chunks)
    bm25_res_raw = bm25_model.top_k_sentence(query, k=50)
    bm25_list = [(res[0], res[2]) for res in bm25_res_raw]

    # --- Step 3: Vector å¬å› ---
    print(">>> [2/4] æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•å¹¶æ£€ç´¢...")
    try:
        docs = [Document(page_content=t) for t in chunks]
        vector_store = FAISS.from_documents(docs, embedding_model)
        vector_res_raw = vector_store.similarity_search_with_score(query, k=50)
        vector_list = [(doc.page_content, score) for doc, score in vector_res_raw]
    except Exception as e:
        print(f"âŒ å‘é‡æ£€ç´¢æ­¥éª¤å‡ºé”™: {e}")
        return []

    # --- Step 4: RRF èåˆ ---
    print(">>> [3/4] æ­£åœ¨æ‰§è¡Œ RRF èåˆ...")
    fusion_results = rrf_fusion(bm25_list, vector_list)

    # å–å‰ 50 ä¸ªå€™é€‰
    candidate_texts = [item[0] for item in fusion_results[:50]]
    if not candidate_texts:
        return []

    # --- Step 5: Rerank é‡æ’åº ---
    print(">>> [4/4] æ­£åœ¨ä½¿ç”¨ BGE-Reranker è¿›è¡Œç²¾æ’ (CPUæ¨¡å¼)...")

    try:
        # æ„é€ è¾“å…¥å¯¹
        rerank_pairs = [[query, doc_text] for doc_text in candidate_texts]

        # ğŸ”¥ã€å…³é”®ä¿®æ”¹ã€‘æ·»åŠ  batch_size=1 å’Œè¿›åº¦æ¡
        # CPU è®¡ç®—èƒ½åŠ›æœ‰é™ï¼Œå¿…é¡»ç”± batch_size=1 æ¥ä¿è¯å†…å­˜å®‰å…¨
        scores = reranker_model.predict(
            rerank_pairs,
            batch_size=1,  # æœ€å®‰å…¨çš„è®¾ç½®
            show_progress_bar=True,  # æ˜¾ç¤ºè¿›åº¦æ¡
            num_workers=0  # é˜²æ­¢å¤šè¿›ç¨‹æ­»é”
        )

        # æ’åº
        scored_results = list(zip(candidate_texts, scores))
        scored_results.sort(key=lambda x: x[1], reverse=True)

        final_top_3 = scored_results[:3]

        formatted_results = []
        for i, (text, score) in enumerate(final_top_3):
            block = f"ã€Rank {i + 1} | Rerankå¾—åˆ†: {score:.4f}ã€‘\n{text}"
            formatted_results.append(block)

        return formatted_results

    except Exception as e:
        print(f"âŒ Rerank é˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}")
        # æ‰“å°è¯¦ç»†å †æ ˆä»¥ä¾¿è°ƒè¯•
        import traceback
        traceback.print_exc()
        return []


def batch_process_md_files(data_path: str = None):
    """
    æ‰¹é‡å¤„ç†Markdownæ–‡ä»¶

    å‚æ•°:
        data_path: æ•°æ®ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•
    """
    # è®¾ç½®æ•°æ®è·¯å¾„
    if data_path is None:
        data_path = '.'  # å½“å‰ç›®å½•

    # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
    md_files = find_md_files(data_path)

    if not md_files:
        print(f"åœ¨ {data_path} ä¸­æœªæ‰¾åˆ°Markdownæ–‡ä»¶")
        return

    print(f"æ‰¾åˆ° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶:")
    for i, file_path in enumerate(md_files, 1):
        print(f"  {i}. {os.path.basename(file_path)}")

    # åŠ è½½æ¨¡å‹
    print("\n" + "=" * 50)
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")

    # æ£€æŸ¥æ˜¯å¦æœ‰GPUå¯ç”¨
    import torch
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-large-zh-v1.5",
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True, 'batch_size': 8}
    )

    # æ ¹æ®è®¾å¤‡è®¾ç½®reranker
    reranker_device = 'cpu'  # ä¸ºäº†ç¨³å®šæ€§ï¼Œrerankeré€šå¸¸ç”¨CPU
    reranker = CrossEncoder(model_name="BAAI/bge-reranker-large", device=reranker_device)
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼")

    # æµ‹è¯•é—®é¢˜
    queries = [
        "å…¬å¸è¥æ”¶",
        "å¹´åº¦è¥ä¸šæ”¶å…¥ã€å‡€åˆ©æ¶¦æ˜¯å¤šå°‘ï¼Ÿ",
        "åˆ©æ¶¦åˆ†é…é¢„æ¡ˆ",
        "äº§é‡ã€é”€é‡æ•°æ®",
        "å‘å±•æˆ˜ç•¥",
        "é£é™©å› ç´ "
    ]

    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for md_file in md_files:
        print("\n" + "â˜…" * 30)
        print(f"å¤„ç†æ–‡ä»¶: {os.path.basename(md_file)}")
        print("â˜…" * 30 + "\n")

        # å…ˆè¿›è¡Œåˆ†å—å¤„ç†
        print(f"æ­£åœ¨å¯¹ {os.path.basename(md_file)} è¿›è¡Œåˆ†å—å¤„ç†...")
        chunks = get_text_chunks_from_md(md_file, save_json=True)

        if not chunks:
            print(f"  âš ï¸  {os.path.basename(md_file)} åˆ†å—å¤±è´¥ï¼Œè·³è¿‡")
            continue

        print(f"  ç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")

        # æ‰§è¡Œæ£€ç´¢æµ‹è¯•
        for i, query in enumerate(queries[:3]):  # åªæµ‹è¯•å‰3ä¸ªé—®é¢˜
            print(f"\n[é—®é¢˜ {i + 1}] >>> {query}")

            results = hybrid_search_md(query, md_file, embeddings, reranker)

            print(f"\n--- é—®é¢˜ {i + 1} çš„ Top-3 æ£€ç´¢ç»“æœ ---")
            if results:
                for res in results:
                    print(res)
                    print("-" * 30)
            else:
                print("æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚")

            print("\n" + "=" * 60)


# ================= ä¸»ç¨‹åºå…¥å£ =================
# ================= ä¼˜åŒ–åçš„ä¸»ç¨‹åºå…¥å£ =================

if __name__ == '__main__':
    # 1. æŒ‡å®šæµ‹è¯•æ–‡ä»¶è·¯å¾„ (å¯ä»¥æ˜¯å•ä¸ªæ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥æ˜¯ç›®å½•)
    test_md_path = './'  # æ›¿æ¢ä¸ºä½ çš„å®é™…æ–‡ä»¶å

    # 2. å®šä¹‰ä½ æƒ³æé—®çš„æŒ‡å®šé‡‘èé—®é¢˜
    financial_queries = [
        "2023å¹´åº¦å…¬å¸çš„è¥ä¸šæ”¶å…¥ã€å‡€åˆ©æ¶¦åŠå…¶åŒæ¯”å¢é•¿ç‡æ˜¯å¤šå°‘ï¼Ÿ",
        "å…¬å¸ä¸»è¦çš„ç»è¥é£é™©æœ‰å“ªäº›ï¼Ÿè¯·åˆ—ä¸¾è‡³å°‘ä¸‰ä¸ªã€‚",
        "å…¬å¸æœ¬å¹´åº¦çš„åˆ©æ¶¦åˆ†é…é¢„æ¡ˆæˆ–è‚¡åˆ©åˆ†é…æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ç ”å‘æŠ•å…¥å è¥ä¸šæ”¶å…¥çš„æ¯”é‡æ˜¯å¤šå°‘ï¼Ÿ",
        "å‰äº”å¤§å®¢æˆ·çš„é”€å”®é¢å æ¯”æƒ…å†µå¦‚ä½•ï¼Ÿ"
    ]

    # 3. åˆå§‹åŒ–æ¨¡å‹ (åªåŠ è½½ä¸€æ¬¡ï¼Œé¿å…å†…å­˜æµªè´¹)
    print("\n" + "=" * 20 + " åˆå§‹åŒ–æ¨¡å‹ " + "=" * 20)
    import torch

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-large-zh-v1.5",
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )

    reranker = CrossEncoder(
        model_name="BAAI/bge-reranker-large",
        device='cpu'  # Reranker åœ¨ CPU ä¸Šè¿è¡Œé€šå¸¸æ›´ç¨³å®šï¼Œè‹¥æ˜¾å­˜å……è¶³å¯æ”¹ä¸º device
    )
    print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}")

    # 4. æ‰§è¡Œå¬å›ä¸æé—®
    if os.path.exists(test_md_path):
        print(f"\nå¼€å§‹åˆ†ææ–‡ä»¶: {os.path.basename(test_md_path)}")

        for idx, query in enumerate(financial_queries):
            print(f"\nğŸ” [é—®é¢˜ {idx + 1}] {query}")

            # è°ƒç”¨æ··åˆæ£€ç´¢å‡½æ•°
            # æ³¨æ„ï¼šè¯¥å‡½æ•°å†…éƒ¨ä¼šè‡ªåŠ¨å®Œæˆï¼šåˆ†å— -> BM25 -> Vector -> RRF -> Rerank
            results = hybrid_search_md(
                query=query,
                md_file_path=test_md_path,
                embedding_model=embeddings,
                reranker_model=reranker
            )

            # 5. æ ¼å¼åŒ–è¾“å‡ºå¬å›çš„å†…å®¹
            if results:
                print(f"âœ… å¬å›æˆåŠŸï¼Œæœ€ç›¸å…³çš„ Top-{len(results)} ä¸ªä¸Šä¸‹æ–‡ç‰‡æ®µå¦‚ä¸‹ï¼š")
                for res in results:
                    print(res)
                    print("-" * 50)
            else:
                print("âŒ æœªèƒ½å¬å›ç›¸å…³å†…å®¹ï¼Œè¯·æ£€æŸ¥åˆ†å—ç­–ç•¥æˆ–æ–‡ä»¶æ ¼å¼ã€‚")
    else:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {test_md_path}")

    print("\n" + "=" * 20 + " æµ‹è¯•æµç¨‹ç»“æŸ " + "=" * 20)